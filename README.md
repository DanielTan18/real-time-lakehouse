# ğŸ§  Real-Time Data Lakehouse with Spark, Kafka & Iceberg

This project demonstrates a **real-time data lakehouse architecture** built using **Apache Spark**, **Apache Kafka**, and **Apache Iceberg**, with **MinIO** as the S3-compatible object store.  
It simulates live transaction streams, processes them in real time with Spark Structured Streaming, and stores them in an Iceberg table for analytics.

---

## ğŸš€ Architecture Overview

![Real-Time Lakehouse](real_time_lakehouse.jpg "Real-Time Lakehouse Architecture")

**Flow Summary:**
1. `transaction_producer.py` continuously generates random financial transactions using the **Faker** library.
2. These messages are published to a Kafka topic named `transactions`.
3. A **Spark Structured Streaming** job consumes this topic in real time.
4. Spark writes the parsed transaction data into an **Iceberg table** stored in **MinIO (S3)**.

---

## ğŸ§° Tech Stack

| Component | Description |
|------------|-------------|
| **Apache Kafka** | Message broker for real-time data streaming |
| **Apache Spark** | Stream processing engine |
| **Apache Iceberg** | Lakehouse table format with ACID guarantees |
| **MinIO** | S3-compatible object storage |
| **Docker Compose** | Container orchestration |
| **Python + Faker** | Synthetic data generation |

---


---

## âš™ï¸ Setup Instructions

### 1ï¸âƒ£ Prerequisites
Ensure you have:
- ğŸ³ **Docker** and **Docker Compose** installed
- ğŸ **Python 3.8+** with `pip install kafka-python faker`

---

## ğŸš€ Step 2: Start All Containers

From the project root directory, run:

```bash
docker-compose up -d
```

This command spins up the following services:

- ğŸ§© **Kafka Broker** â€” message streaming platform  
- âš™ï¸ **Spark Master & Worker** â€” distributed processing engine  
- ğŸ’¾ **MinIO Object Storage** â€” S3-compatible data lake storage  

### âœ… Verify the Services

You can confirm that everything is running by visiting the following URLs:

| Service | URL | Default Credentials |
|----------|-----|---------------------|
| **Spark Master UI** | [http://localhost:8080](http://localhost:8080) | â€” |
| **MinIO Console** | [http://localhost:9001](http://localhost:9001) | **Username:** `minio` <br> **Password:** `minio123` |

---

## ğŸ’¸ Step 3: Start the Transaction Producer

This script simulates live transactions and streams them to **Kafka** every second.

Run the producer script:

```bash
python transaction_producer.py
```

You should start seeing logs like:

```
Sent: {'txn_id': '...', 'user_id': 1423, 'amount': 134.22, 'currency': 'MYR', ...}
```


## âš¡ Step 4: Start Spark Structured Streaming

At the same time, open another terminal and run the following command **inside the Spark container** to start the stream:

```bash
docker exec -it spark \
  /opt/spark/bin/spark-submit \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  /app/spark_to_iceberg.py
```

This will:

- ğŸ”„ Read data from **Kafka** (`transactions` topic)  
- ğŸ§® Parse and transform **JSON** messages  
- ğŸ§Š Write them continuously to an **Iceberg table** stored in **MinIO**

---

## ğŸ” Step 5: Verify Iceberg Data

To inspect the ingested data, open a **PySpark shell** inside the Spark container:

```bash
docker exec -it spark /opt/spark/bin/pyspark \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
```

Then run the following query:

```python
spark.sql("SELECT * FROM local.default.transactions LIMIT 10").show()
```

You should see live transactions being stored in your Iceberg table.

---

## ğŸ§  How It Works

**Data Flow Overview**

```
Kafka â†’ Spark Structured Streaming â†’ Iceberg â†’ MinIO
```

**Explanation**

- âš™ï¸ **Kafka â†’ Spark Streaming**: Spark continuously polls Kafka for new messages.  
- ğŸ§® **Spark â†’ Iceberg**: Each micro-batch is written atomically into an Iceberg table.  
- ğŸ’¾ **Iceberg â†’ MinIO**: Iceberg metadata and Parquet files are stored in MinIO.  
- ğŸ§¬ **Schema Evolution**: Iceberg supports schema changes without rewriting data.  
- ğŸ”’ **ACID Guarantees**: Every write operation is consistent, isolated, and reliable.


## âš¡ Quick Start (TL;DR)

Get the entire pipeline running in just a few commands ğŸš€

```bash
# 1ï¸âƒ£ Start all containers
docker-compose up -d

# 2ï¸âƒ£ Start producing transactions
python transaction_producer.py

# 3ï¸âƒ£ Start Spark stream
docker exec -it spark \
  /opt/spark/bin/spark-submit \
  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
  /app/spark_to_iceberg.py
```
